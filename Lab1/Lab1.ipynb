{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Используем алгоритм Витерби"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала попробуем на немецком языке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#импорт библиотек\n",
    "import conllu \n",
    "from io import open\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#парсим документ\n",
    "doc_train = conllu.parse_incr(open(\"de_gsd-ud-train.conllu\",'r',encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#С помощью словарей считаем количество n-граммов\n",
    "c3 = {} # 3-граммов \n",
    "c2 = {} # биграммов\n",
    "c1 = {} # монограммов\n",
    "cT = {} # пар слово-тэг\n",
    "for tokenlist in doc_train:\n",
    "    sentence = [[\"*\",\"*\"]] + [[\"*\",\"*\"]] + [[tokenlist[i][\"lemma\"], tokenlist[i][\"upostag\"]]\n",
    "        for i in range(len(tokenlist)) ] + [[\"STOP\",\"STOP\"]]\n",
    "\n",
    "    for i in range(0, len(sentence)-2):\n",
    "        #вернем 0, если триграмма нет в словаре \n",
    "        c3old = c3.get( (sentence[i][1], sentence[i+1][1], sentence[i+2][1]), 0)\n",
    "        #а если есть, увеличиваем счетчик на 1\n",
    "        c3.update( { (sentence[i][1], sentence[i+1][1], sentence[i+2][1]): c3old+1 } )\n",
    "\n",
    "    for i in range(0, len(sentence)-1):\n",
    "        c2old = c2.get( (sentence[i][1], sentence[i+1][1]), 0)\n",
    "        c2.update( { (sentence[i][1], sentence[i+1][1]): c2old+1 } )\n",
    "\n",
    "    for i in range(0, len(sentence)):\n",
    "        c1old = c1.get( (sentence[i][1]), 0)\n",
    "        c1.update( { (sentence[i][1]): c1old+1 } )\n",
    "\n",
    "    for i in range(0, len(sentence)):\n",
    "        cTold = cT.get( (sentence[i][1], sentence[i][0]), 0)\n",
    "        cT.update( { (sentence[i][1], sentence[i][0]): cTold+1 } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2686\n",
      "267\n",
      "19\n",
      "43985\n"
     ]
    }
   ],
   "source": [
    "#для удобства бросаем данные в pickle\n",
    "print (len(c3))\n",
    "print (len(c2))\n",
    "print (len(c1))\n",
    "print (len(cT))\n",
    "\n",
    "with open('cs_german.pkl', 'wb') as f:\n",
    "    pickle.dump([cT, c1, c2, c3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#возвращаем данные обратно\n",
    "with open('cs_german.pkl', 'rb') as f:\n",
    "    cT, c1, c2, c3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#делаем список уникальных тэгов\n",
    "tags = list(c1.keys())\n",
    "tags.remove('*')\n",
    "tags.remove('STOP')\n",
    "#счетчик уникальных тэгов\n",
    "k = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'PRON', 'PUNCT', 'VERB', 'NOUN', 'ADV', 'SYM', 'DET', 'PART', 'NUM', '_', 'AUX', 'ADJ', 'CCONJ', 'X', 'SCONJ', 'ADP']\n"
     ]
    }
   ],
   "source": [
    "#вспомогательные функции из алгоритма Витерби\n",
    "def q(s, u, v):\n",
    "    eps = 1e-7\n",
    "    return c3.get((u, v, s), eps) / c2.get((u, v), eps)\n",
    "\n",
    "\n",
    "def e(x, s):\n",
    "    eps = 1e-7\n",
    "    return cT.get((s, x), eps) / c1.get((s), eps)\n",
    "\n",
    "#прогнозирует тэги в зависимости от положения слова в предложении\n",
    "def K(n): \n",
    "    if n == -1 or n == 0:\n",
    "        return ['*']\n",
    "    else:\n",
    "        return tags\n",
    "\n",
    "print(K(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм Витерби в форме функции от токенизированного предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(sent):\n",
    "    Pi = {(0, '*', '*'): 1}\n",
    "    bp = {}\n",
    "    n = len(sent) - 2\n",
    "    y = [\"\"] * (n + 1)\n",
    "\n",
    "    for k in range(1, n + 1):\n",
    "        xk = sent[k]\n",
    "       \n",
    "        for u in K(k-1):\n",
    "            for v in K(k):\n",
    "                w = K(k-2)\n",
    "                v1 = map(lambda wi:\n",
    "                            Pi.get((k-1, wi, u)) *\n",
    "                            q(v, wi, u) *\n",
    "                            e(xk, v), w)\n",
    "                v1 = list(v1)\n",
    "                PiNew = max(v1)\n",
    "                bpNew = w[v1.index(PiNew)]\n",
    "                Pi.update({(k, u, v): PiNew})\n",
    "                bp.update({(k, u, v): bpNew})\n",
    "\n",
    "    v2 = {}\n",
    "    for u in K(n-1):\n",
    "        for v in K(n):\n",
    "            v2.update({(n, u, v): Pi.get((n, u, v)) * q(\"STOP\", u, v)})\n",
    "\n",
    "    v2max = max(list(v2.values()))\n",
    "    for (n, u, v), val in v2.items():\n",
    "        if val == v2max:\n",
    "            (y[n-1], y[n]) = (u,v)\n",
    "\n",
    "    for k in range(n-2, 0, -1):\n",
    "        y[k] = bp.get((k + 2, y[k+1], y[k+2]))\n",
    "    #возвращает список тэгов\n",
    "    return y[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогоняем на тестовом корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_test = conllu.parse_incr(open(\"de_gsd-ud-train.conllu\",'r',encoding = \"utf-8\"))\n",
    "test_tags = [] # фактические тэги\n",
    "predict_tags = [] # предсказанные тэги\n",
    "\n",
    "for tokenlist in doc_test:\n",
    "    # получаем предложение\n",
    "    sentence=[\"*\"] + [tokenlist[i][\"lemma\"] for i in range(len(tokenlist)) ] + [\"STOP\"]\n",
    "    # получаем оригинальные теги\n",
    "    test_tags.append([tokenlist[i][\"upostag\"] for i in range(len(tokenlist)) ])\n",
    "    # получаем предсказанные таги\n",
    "    predict_tags.append(viterbi(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оцениваем ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error =  0.0353555743462\n"
     ]
    }
   ],
   "source": [
    "err = 0.\n",
    "for tag in range(len(test_tags)):\n",
    "    err += np.mean(np.array(test_tags[tag]) != np.array(predict_tags[tag]))\n",
    "print('Error = ', err/len(test_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Время усложнить задачу: посмотрим на работу алгоритма в рамках индонезийского. Шаги аналогичны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#парсим документ\n",
    "doc_train = conllu.parse_incr(open(\"id_gsd-ud-train.conllu\",'r',encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#С помощью словарей считаем количество n-граммов\n",
    "n3 = {} # 3-граммов \n",
    "n2 = {} # биграммов\n",
    "n1 = {} # монограммов\n",
    "nT = {} # пар слово-тэг\n",
    "for tokenlist in doc_train:\n",
    "    sentence = [[\"*\",\"*\"]] + [[\"*\",\"*\"]] + [[tokenlist[i][\"lemma\"], tokenlist[i][\"upostag\"]]\n",
    "        for i in range(len(tokenlist)) ] + [[\"STOP\",\"STOP\"]]\n",
    "\n",
    "    for i in range(0, len(sentence)-2):\n",
    "        #вернем 0, если триграмма нет в словаре \n",
    "        n3old = n3.get( (sentence[i][1], sentence[i+1][1], sentence[i+2][1]), 0)\n",
    "        #а если есть, увеличиваем счетчик на 1\n",
    "        n3.update( { (sentence[i][1], sentence[i+1][1], sentence[i+2][1]): n3old+1 } )\n",
    "\n",
    "    for i in range(0, len(sentence)-1):\n",
    "        n2old = n2.get( (sentence[i][1], sentence[i+1][1]), 0)\n",
    "        n2.update( { (sentence[i][1], sentence[i+1][1]): n2old+1 } )\n",
    "\n",
    "    for i in range(0, len(sentence)):\n",
    "        n1old = n1.get( (sentence[i][1]), 0)\n",
    "        n1.update( { (sentence[i][1]): n1old+1 } )\n",
    "\n",
    "    for i in range(0, len(sentence)):\n",
    "        nTold = nT.get( (sentence[i][1], sentence[i][0]), 0)\n",
    "        nT.update( { (sentence[i][1], sentence[i][0]): nTold+1 } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2256\n",
      "248\n",
      "18\n",
      "18981\n"
     ]
    }
   ],
   "source": [
    "#для удобства бросаем данные в pickle\n",
    "print (len(n3))\n",
    "print (len(n2))\n",
    "print (len(n1))\n",
    "print (len(nT))\n",
    "\n",
    "with open('cs_id.pkl', 'wb') as f:\n",
    "    pickle.dump([nT, n1, n2, n3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#возвращаем данные обратно\n",
    "with open('cs_id.pkl', 'rb') as f:\n",
    "    nT, n1, n2, n3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#делаем список уникальных тэгов\n",
    "tags_id = list(n1.keys())\n",
    "tags_id.remove('*')\n",
    "tags_id.remove('STOP')\n",
    "#счетчик уникальных тэгов\n",
    "k = len(tags_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_test = conllu.parse_incr(open(\"id_gsd-ud-train.conllu\",'r',encoding = \"utf-8\"))\n",
    "test_tags_id = [] # фактические тэги\n",
    "predict_tags_id = [] # предсказанные тэги\n",
    "\n",
    "for tokenlist in doc_test:\n",
    "    # получаем предложение\n",
    "    sentence=[\"*\"] + [tokenlist[i][\"lemma\"] for i in range(len(tokenlist)) ] + [\"STOP\"]\n",
    "    # получаем оригинальные теги\n",
    "    test_tags_id.append([tokenlist[i][\"upostag\"] for i in range(len(tokenlist)) ])\n",
    "    # получаем предсказанные таги\n",
    "    predict_tags_id.append(viterbi(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error =  0.848191920346\n"
     ]
    }
   ],
   "source": [
    "err = 0.\n",
    "for tag in range(len(test_tags_id)):\n",
    "    err += np.mean(np.array(test_tags_id[tag]) != np.array(predict_tags_id[tag]))\n",
    "print('Error = ', err/len(test_tags_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеем следующие результаты: на немецком корпусе ошибка составляет около 0.035, на индонезийском же ошибка значительно выше (около 0.85). Причиной тому можно считать специфичность и сложность индонезийского (отсутствие спряжений у глаголов, словообразование реализуется с помощью повторения слов, семья языка многочисленна). Алгоритм отрабатывает довольно долго (со второго раза нашлась машина, которая прогнала код). В будущем надеюсь найти оптимизированную библиотеку с реализацией алгори"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
